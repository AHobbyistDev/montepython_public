\documentclass[10pt]{article}
\usepackage{url}
\usepackage{alltt}
\usepackage{hyperref}
\usepackage{fullpage}
\newcommand{\CLASS}{\texttt{CLASS}}
\newcommand{\MP}{\texttt{Monte Python}}
\newcommand{\Wlik}{\texttt{Wlik}}
\newcommand{\Clik}{\texttt{Clik}}
\newcommand{\tild}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}


\title{ {\huge \bf Monte Python Documentation}\\Release v1.0}
\author{Benjamin Audren \& Julien Lesgourgues}

\begin{document}
\maketitle

\newpage

\tableofcontents

\newpage

\section{Prerequisites}

  \subsection{Python}

  First of all, you need a clean installation of {\bf python} (version $2.x$,
  $x\geq7$, but $<3.0$), with at least the {\bf numpy module} (version
  $\geq1.4.1$) and the {\bf cython module}. This last one is to convert the C
  code \CLASS~into a Python class.\par

  If you also want the output plot to have cubic interpolation for analyzing
  chains, you should also have the {\bf scipy module} (at least version $0.9.0$).
  %with the method \emph{interpolate}. 
  In case this one is badly installed, you
  will have an error message when running the analyze module of \MP,
  and obtain only linear interpolation. Though not fatal, this problem produces
  ugly plots.\par

To test for the presence of the modules {\bf numpy},  {\bf scipy},  {\bf cython} on your machine, you can type
  \begin{alltt}
   \$ python
   \$ >>> import numpy
   \$ >>> import scipy
   \$ >>> import cython
   \$ >>> exit()
  \end{alltt}
 If one of these steps fails, go to the corresponding websites, and follow the instructions (if you have the honour to have the root password on your machine, an \verb?apt-get install python-numpy?, \verb?python-scipy? and \verb?cython? will do the trick.).

  Note that you can use the code with Python 2.6 also, even though you need to
  download two packages separately ({\bf ordereddict} and {\bf argparse}). For this, it is
  just a matter of downloading the two files (\verb?ordereddict.py? and \verb?argparse.py?),
  and placing them in your code directory without installation steps.\par

  \subsection{\CLASS}

  Next in line, you must compile the python wrapper of \CLASS. Download the
  latest version ($\geq 1.5.0$) at \url{http://class-code.net}, and follow the basic
  instruction. Instead of  \verb?make class?, type \verb?make?. This will
  also create an archiv .ar of the code, useful in the next step. After this, do:

  \begin{alltt}
   class]\$ cd python/
   python]\$ python setup.py build
   python]\$ python setup.py install --user
  \end{alltt}

  If you have correctly installed cython, this should add Classy as a new python
  module. You can check the success of this operation by running the following
  command:

  \begin{alltt}
    \tild]\$ python
    >>> from classy import Class
  \end{alltt}

If the installation was successfull, this should work within any directory. If you get no error message from this line, you know everything is fine.

  If at some point you have several different coexisting versions of \CLASS~on the system, and you are worried that \MP~is not using the good one, rest
  reassured. As long as you run \MP~with the proper path to the proper \CLASS~in your configuration file (see section \ref{sec:installation}: Installation),
  then it will use this one.
  
%%%%%%%% INSTALLATION %%%%%%%%
\section{Installation\label{sec:installation}}

\subsection{Main code}

  Move the latest release of \MP~to one of your folders, called e.g. \verb?code/? (for instance, this could be the folder containing also \verb?class/?), and untar its
  content:

  \begin{alltt} 
    code]\$ bunzip montepython-v1.0.0.tar.bz2 
    code]\$ tar -xvf montepython-v1.0.0.tar
    code]\$ cd montepython
  \end{alltt}

 You will have to edit two files (the first, once for every new distribution of \MP, and
 the second, once and for all). The first to edit is
 \verb?code/MontePython.py?. Its first line reads:

 \begin{alltt}
    #!/usr/bin/python
 \end{alltt}

 You should eventually replace this path with the one of your python 2.7 executable, if different.
 This modification is not crucial, it simply allows to run the code by simply typing \verb?code/Montepython.py?.
 If, instead, you run it through python (\emph{i.e.}: \verb?python?
 \verb?code/MontePython.py?), then this line will be disregarded.

 The second file to change, and this one is crucial, is \verb?default.conf?, in
 the root directory of the code. This file will tell \MP\, where your
 other programs (in particular \CLASS) are installed, and where you are storing the data for the
 likelihoods. It will be interpreted as a python file, so be careful to
 reproduce the syntax exactly. At minimum, {\bf default.conf} should contain one line, filled with the path of your \verb?class/? directory:
 \begin{alltt}
   path['cosmo']   = 'path/to/your/class/'
 \end{alltt}
 
 %For members of the Planck collaboration only: if you have installed Clik, then you should also add:
 %\begin{alltt}
 %  path['clik']    = 'path/to/your/clik/examples/'
 %\end{alltt}
 
 To check that \MP~is ready to work, simply type \verb?python code/MontePython.py --help? (or just \verb?code/MontePython.py --help?). This will provide you with a short description of the available command line arguments, explained in section \ref{commands}. 

 \subsection{WMAP 7 likelihood}

  To use the likelihood of WMAP, we propose a python wrapper, located in the
  \verb?wrapper_wmap? directory. Just like with the \CLASS~wrapper, you need to
  install it, although the procedure differs. Go to the wrapper directory, and enter:

  \begin{alltt}
    wrapper_wmap]\$ ./waf configure install_all_deps
  \end{alltt}

  This should read the configuration of your distribution, and install the WMAP likelihood code and its
  dependencies (cfitsio) automatically on your machine. For our purpose,
  though, we prefer using the intel mkl libraries, which are much faster. To
  tell the code about your local installation of mkl libraries, please add to the line above some options:
  \begin{alltt}
     --lapack_mkl=/path/to/intel/mkl/10.3.8 --lapack_mkl_version=10.3
  \end{alltt}

  Once the configuration is done properly, finalize the installation by typing:
  
  \begin{alltt}
    wrapper_wmap]\$ ./waf install
  \end{alltt}

  The code will generate a configuration file, that you will need to source
  before using the WMAP likelihood with \MP. The file is \verb?clik_profile.sh?, and is located
  in \verb?wrapper_wmap/bin/?. So if you want to use the likelihood \verb?`wmap'?, before any call to \MP~(or inside your
  scripts), you should execute

  \begin{alltt}
    \tild]\$ source /path/to/MontePython/wrapper_wmap/bin/clik_profile.sh
  \end{alltt}

The wrapper will use the original version of the WMAP likelihood codes downloaded and placed in the folder \verb?wrapper_wmap/src/likelihood_v4p1/? during the installation process. This likelihood will be compiled later, when you will call it for the first time from the \MP~code. Before calling it for the first time, you could eventually download the WMAP patch from Wayne Hu's web site, for a faster likelihood.

You should finally download the WMAP data files by yourself, place them anywhere on your system, and specify the path to these data files in the file \verb?likelihoods/wmap/wmap.data?.

\section{Getting started}

\subsection{Input parameter file}

An example of input parameter file is provided with the download package, under the name \verb?example.param?. Input files are organised as follows:

  \begin{alltt}
    data.experiments = ['experiment1', 'experiment2', ...]
    
    data.parameters['cosmo_name']       = [mean, min, max, sigma, scale, 'cosmo']
    ...

    data.parameters['nuisance_name']    = [mean, min, max, sigma, scale, 'nuisance']
    ...

    data.parameters['cosmo_name']       = [mean, min, max, sigma, scale, 'derived']
    ...

    data.cosmo_arguments['cosmo_name']           = value

    data.N = 10
    data.write_step = 5
  \end{alltt}

  The first command is rather explicit. You will list there all the experiments
  you want to take into account. Their name should coincide with the name of one of the several sub-directories in the \verb?likelihood/? directory. Likelihoods will be explained in section \ref{likelihoods}\\

  In \verb?data.parameters?, you can list all the cosmo and nuisance parameter that you
  want to vary in the Markov chains. For each of them you must give an array
  with six elements, in this order: 
  \begin{itemize}
    \item {\bf mean value} (your guess for the best fitting value, from which the first jump will start)
    \item {\bf minimum value} (set to $-1$ for unbounded prior edge), 
    \item {\bf maximum value} (set to $-1$ for unbounded prior edge), 
    \item {\bf sigma} (your guess for the standard deviation of the posterior of this parameter, its square will be used as the variance of the proposal density when there is no covariance matrix including this parameter passed as an input),
    \item {\bf scale} (most of the time, it will be $1$, but occasionnaly you can use a rescaling factor for convenience, for instance {\tt 1.e-9} if you are dealing with \verb?A_s? or \verb?0.01? if you are dealing with \verb?omega_b?) 
    \item {\bf role} ({\tt 'cosmo'} for MCMC parameters used by the Boltzmann code, {\tt 'nuisance'} for MCMC parameters used only by the likelihoods, and {\tt 'derived'} for parameters not directly varied by the MCMC algorithm, but to be kept in the chains for memory).
      \end{itemize}
  
  In  \verb?data.cosmo_arguments?, you can pass to the Boltzmann code any parameter that you want to fix to a non-default value (cosmological parameter, precision parameter, flag, name of input file needed by the Bolztmann code, etc.). The names and values should be the same as in a \CLASS~input file, so the values can be numbers or a strings, e.g:
  \begin{alltt}
    data.cosmo_arguments['Y_He']           = 0.25
    \end{alltt}  
    or
  \begin{alltt}
    data.cosmo_arguments['Y_He']           = 'BBN'    
    data.cosmo_arguments['sBBN file'] = data.path['cosmo']+'/bbn/sBBN.dat'
    \end{alltt}
  
  All elements you input with a \verb?cosmo?, \verb?derived? or \verb?cosmo_arguments? role will
  be interpreted by the cosmological code (only \CLASS~ so far). They are not coded anywhere inside \MP. \MP~takes parameter names, assigns values, and passes all of these to \CLASS~as if they were written in a \CLASS~input file. The advantages of this scheme are obvious. If you need to fix or vary whatever parameter known by \CLASS, you don't need to edit \MP, you only need to write these parameters in the input parameter file. Also, \CLASS~is able to interpret input parameters from a \CLASS~input file with a layer of simple logic, allowing to specify different parameter combinations. Parameters passed from the parameter file of \MP~go through the same layer of logic. 
  
  If a \verb?cosmo?, \verb?derived? or \verb?cosmo_arguments? parameter is not understood by the Boltzmann code, \MP~will stop and return an explicit error message. A similar error will occur if one of the likelihoods requires a \verb?nuisance? parameter that is not passed in the list.

  You may wish occasionally to use in the MCMC runs a new parameter that is not a \CLASS~ parameter, but can be mapped to one or several \CLASS~parameters (e.g. you may wish to use in your chains $\log(10^{10}A_s)$ instead of $A_s$). There is a function, located in \verb?code/data.py?, that you can edit to define such mappings. It is called  \verb?update_cosmo_arguments?. Before calling \CLASS, this function will simply substitute in the list of arguments your customized parameters by some \CLASS~parameters. Several exemple of such mappings are already implemented, allowing you for instance to use 
\verb?'Omega_Lambda'?, \verb?'ln10^{10}A_s'? or \verb?'exp_m_2_tau_As'? in your chains. Looking at these examples, the user can easily write new ones even without knowing python.
  
  The last two lines of the input parameter file are the number of steps you want your chain to contain
  (\verb?data.N?) and the number of accepted steps the system should wait
  before writing it down to a file (\verb?data.write_step?). Typically, you
  will need a rather low number here, e.g. \verb?data.write_step = 5? or \verb?10?. The reason for not setting this parameter to one is just to save a bit of time in writing on the disk.
 
  In general, you will want to specify the number of steps in the command line,
  with the option \verb?-N? (see section~\ref{commands}). This will overwrite the value passed in the input parameter file. The value by default in the parameter file, \verb?data.N = 10?,
  is intentionnaly low, 
  simply to prevent doing any mistake while testing the program on a cluster.


\subsection{Output directory}

You are assumed to use the code in the following way: for every set of
  experiments and parameters you want to test, including different priors, some
  parameters fixed, etc\ldots you should use one output folder. This way, the folder
  will keep track of the exact calling of the code, allowing you to reproduce
  the data at later times, or to complete the existing chains. All 
  important data are stored in your \verb?folder/log.param? file.\\

  Incidentaly, if you are starting the program in an existing folder, already
  containing a \verb?log.param? file, then you do not even have to specify a
  parameter file: the code will use it automatically. This will avoid mixing
  things up. If you are using one anyway, the code will first check whether the
  two parameter files are in agreement, and if not, will stop and say so. In that way, you are sure not to put together some chains obtained under different physical assumptions.\\
  
  In the folder \verb?montepyhton?, you can create a folder \verb?chains? where you will organize your runs e.g. in the following way:
    \begin{alltt}
    montepython/chains/set_of_experiments1/model1
    montepython/chains/set_of_experiments1/model2
    ...
    montepython/chains/set_of_experiments2/model1
    montepython/chains/set_of_experiments2/model2    
    ...
  \end{alltt}
  
  The minimum amount of command lines for running \MP~is an input file, an output directory and a configuration file: if you have already edited \verb?defaut.conf? or copied it to your own \verb?my-machine.conf?, you may already try a mini-run with the command 
    \begin{alltt}
    montepython]\$ code/MontePython.py -conf my-machine.conf -p example.param -o test
    \end{alltt}

\subsection{Analyzing chains and plotting}

Once you have accumulated a few chains, you can analyse the run to get convergence estimates, best-fit values, minimum credible intervals, a covariance matrix  and some plots of the marginalised posterior probability. You can run again \MP~with the \verb?-info? prefix followed by the name of a directory or of several chains, e.g.
\verb?-info chains/myrun/? or \verb?-info chains/myrun/2012-10-26* chains/myrun/2012-10-27*?.
There is no need to pass an input file with parameter names since they have all been stores in the \verb?log.param?.

Information on the acceptance rate and minimum $-\log{\cal L}=\chi^2_{\rm eff}/2$ is written in \verb?chains/myrun/myrun.log?. Information on the convergence (Raferty \& Lewis test for each chain paramater), on the best fit, mean and minimum credible interval for each parameter at the 68.26\%, 95.4\%, 99.7\% level are written in horizontal presentation in \verb?chains/myrun/myrun.h_info?, and in vertical presentation in \verb?chains/myrun/myrun.v_info? (without 99.7\% in the vertical one). A latex file to produce a table with parameter names, means and 68\% errors in written in \verb?chains/myrun/myrun.tex?.

The covariance matrix of the run is written in \verb?chains/myrun/myrun.covmat?. It can be used as an input for the proposal density in a future run. The first line, containing the parameter name, will be read when the covariance matrix will be passed in input. This means that 
the list of parameters in the input covariance matrix and in the run don't need to coincide: the code will automatically eliminate, add and reorder parameters. Note that the rescaling factors passed in the input file are used internally during the run and also in the presentation of results in the \verb?.h_info?, \verb?.v_info?, \verb?.tex? files, but not in the covariance matrix file, which refers to the true parameters.

The 1D posteriors and 2D posterior contours are plotted in \verb?chains/myrun/plots/myrun_1D.pdf? and \verb?chains/myrun/plots/myrun_triangle.pdf?. You will find in section~\ref{commands} a list of commands to customize the plots. Besides, you may wish to change font sizes: for the moment this has to be done by editing the very last lines of \verb?code/analyse.py? and writing your own values for the \verb?fontsize? and \verb?ticksize? variables.

When the chains are not very converged and the posterior probability has local maxima, the code will fail to compute minimum credible intervals and say it in a warning. The two solutions are either to re-run and increase the number of samples, or maybe just to decrease the number of bins with the \verb?-bins? option.

\subsection{Global running strategy}
     
In the current version of \MP, we deliberately  choose not to use MPI communication between instances of the code. Indeed the use of MPI usually makes the installation step more complicated, and the gain is, in our opinion, not worth it. Several chains are launched as individual serial runs (if each instance of \MP~is launched on several cores, \CLASS~and the WMAP likelihood will parallelize since they use OpenMP). They can be run with the same command since chain names  are created automatically with different numbers for each chain: the chain names are in  the form
\verb?yyyy-mm-dd_N__i.txt? where {\tt yyy} is the year, {\tt mm} the month, {\tt dd} the day, {\tt N} the requested number of steps and {\tt i} the smallest available integer at the time of starting a new run.

However the absence of communication between chains implies that the proposal density cannot be updated automatically during the initial stage of a run. Hence the usual strategy consists in launching a first run with a poor (or no) covariance matrix, and a low acceptance rate; then to analyze this run and produce a better covariance matrix; and then to launch a new run with high acceptance rate, leading to nice plots. Remember that in order to respect strictly markovianity and the Metropolis Hastings algorithm, one should not mix up chains produced with different covariance matrices: this is easy if one takes advantage of the \verb?-info? syntax, for example \verb?-info chains/myrun/2012-10-26_10000*?. However mixing runs that started from very similar covariance matrices is harmless.

It is also possible to run on several desktops instead of a single cluster. Each desktop should have a copy of the output folder and with the same \verb?log.param? file, and after running the chains can be grouped on a single machine and analyse. In this case, take care of avoiding that chains are produced with the same name (easy to ensure with either the \verb?-N? or \verb?-chain_number? options). This is a good occasion to keep the desktops of your department finally busy.
     
\section{Command line arguments\label{commands}}

The command \verb?code/MontePython.py --help? lists all command line arguments understood by \MP. They fall in two categories:
\begin{itemize}
\item arguments  useful for running chains:\\
\begin{tabular}{ll}
\verb?-N steps? & number of steps in the chain (when running on a cluster,\\& your run might be stopped before reaching this number)\\
\verb?-o output_folder? &for example \verb?-o chains/myexperiments/mymodel?\\
\verb?-p input_param_file?&for example \verb?-p input/exoticmodel.param?\\
\verb?-c input_cov_matrix?& name of a covariance matrix (created when analyzing a previous run,\\&and used in input to initialise the proposal density). \\&List of parameters in the input covariance matrix \\&and in the run don't need to coincide.\\
\verb?-j jumping_method? & can take two values, \verb?global? (default) and \verb?sequential?.\\& With the global method the code generates a new random direction \\& at each step, with the sequential one it cycles over the eigenvectors of \\&the proposal density (= input covariance matrix). With the global method\\& the acceptance rate is usually lower but the points in the chains are less\\& correlated.  We recommend using the sequential method to get started in\\& difficult cases, when the proposal density is very bad, in order to \\& accumulate points and generate a covariance matrix to be used later \\&with the default jumping method.\\
\verb?-f jumping_factor? & the proposal density is given by the input covariance matrix \\&(or a diagonal matrix with elements given by the square of the input \\&sigma's) multiplied by the square of this factor. In other words, a \\& typical jump will have an amplitude given by sigma times this factor. \\&The default is the famous factor 2.4, found by Dunkely et al. to be\\& an optimal trade-off between high acceptance rate and high correlation\\& of chain elements, at least for multivariate gaussian posterior probabilities.\\& It can be a good idea to reduce this factor for very non-gaussian posteriors. \\&Using \verb?-f 0 -N 1? is a convenient way to get the likelihood exactly at the \\& starting point passed in input. \\
\verb?-conf configuration_file? & as explained before, this command is crucial to tell \MP\\&about the location of \CLASS.\\
\verb?-chain_number chain_number? & chains are named automatically \verb?yyyy-mm-dd_N__i.txt? where {\tt yyy}\\& is the year, {\tt mm} the month, {\tt dd} the day, {\tt N} the requested number of steps\\& and {\tt i} the smallest available integer at the time of starting a new run:\\& so running \MP~several times with exactly the same command\\& will automatically lead to different chain names. This option is a way to \\&enforce a particular number {\tt i}. This can be useful when running on a\\& cluster: for instance you may ask your script to use the job number as {\tt i}. \\
\verb?-r chain_name? & restart from the last point of a previous chain, to avoid a new\\& burn-in stage. At the beginning of the run, the previous chain\\& will be deleted, and its content transfered to the beginning of\\& the new chain.
\end{tabular}
\item arguments  useful for analyzing chains and plotting:\\
\begin{tabular}{ll}
\verb?-info [file [file ...]]?& chains to analyze; give the folder name to analyze all chains in this folder,\\& or individual chains, for example \verb?-info chains/myrun/?\\& or \verb?-info chains/myrun/2012-10-26* chains/myrun/2012-10-27*?\\
\verb?-bins number_of_bins? &  number of bins in the histograms used to derive posterior probabilities\\& and credible intervals, default is 20; reduce this number for smoother\\& plots at the expense of masking details\\
\verb?-no_mean? &by default, when plotting marginalised 1D posteriors, the code also shows the\\& mean likelihood per bin with dashed lines; this option switches off the dashed lines\\
\verb?-comp comparison_folder? & pass the name of another folder (or another set of chains, same syntax as \verb?-info?)\\& if you want to compare 1D posteriors on the same plot. The lists of parameters\\& in the two folders to compare do not need to coincide. Limited so far to two \\&folders to compare in total.\\
\verb?-extra plot_file? & name of an optional file containing a few lines for customizing the plots,\\& \verb?info.to_change={'oldname1':'newname1','oldname2':'newname2',...}?\\&
\verb?info.to_plot=['name1','name2','newname3',...]?\\& 
\verb?infot.new_scales={'name1':number1,'name2':number2,...}?\\
\verb?-noplot? & analyze chains without drawing plots \\
\verb?-all? & by default, produces only full 1D plot and triangle plots, with this option\\& all individual 1D plot and 2D contour plot are stored 
\end{tabular}
\end{itemize}

\section{Example of a complete work session}

I just downloaded and installed \MP, read the previous pages, and I wish to launch and analyse my first run.

I can first create a few folders in order to keep my \verb?montepython? directory tidy in the future. I do a \\
\begin{tabular}{ll}
\verb?$ mkdir chains? &for storing all my chains\\
\verb?$ mkdir chains/planck?& if the first run I want to launch is based on the fake planck likelihood proposed\\& in the \verb?example.param? file\\
\verb?$ mkdir input?& for storing all my input files\\
\verb?$ mkdir scripts?& for storing all my scripts for running the code in batch mode\\
\verb?$ mkdir plot_files?&to store files used to customize plots
\end{tabular}\\

I then copy \verb?example.param? in my input folder, with a name of my choice, e.g. \verb?lcdm.param?, and edit it if needed:
\begin{alltt}
\$ cp example.param input/lcdm.param
\end{alltt}
I then launch a short chain with
 \begin{alltt}
\$ code/Montepython.py -conf my-conf.conf -p input/lcdm.param\\ \mbox{    }-o chains/planck/lcdm -N 5
\end{alltt}
I can see on the screen the evolution of the initialization of the code. At the end I check that I have a 
chain and a \verb?log.param? written in my \verb?chains/planck/lcdm/log.param? directory. I can immediately repeat the experience with the same command. The second chain is automatically created with number 2 instead of 1. I can also run again without the input file:
\begin{alltt}
\$ code/Montepython.py -o chains/planck/lcdm -N 5
\end{alltt}
This works equally well because all information is taken from the \verb?log.param? file.

In some cases, initally, I don't have a covariance matrix to pass in
input\footnote{If I am also a CosmoMC user, I might have an adequate covmat to
start with, before using the covmat that \MP~will produce. Fot this I just need
to edit the first line, add comas between paramater names, and for parameter
that are identical to those in my run, replace CosmoMC parameter names with
equivalent \CLASS~parameter names.}. But in this particular example I can try
the one delivered with the \MP~package, in the \verb?covmat/? directory:
\begin{alltt}
  \$ code/Moncode/Montepython.py -conf my-conf.conf -p input/lcdm.param\\ \mbox{    } -o chains/planck/lcdm -c covmat/fake_planck_lcdm.covmat -N 5
\end{alltt}

%I don't have yet a covariance matrix to pass in input\footnote{If I am also a CosmoMC user, I might have an adequate covmat to start with, before using the covmat that \MP~will produce. Fot this I just need to edit the first line, add comas between paramater names, and for parameter that are identical to those in my run, replace CosmoMC parameter names with equivalent \CLASS~parameter names.}, otherwise I would have run with 
%\begin{alltt}
%\$ code/Montepython.py -conf my-conf.conf -p input/lcdm.param\\ \mbox{    } -o chains/planck/lcdm -c mycovmat.covmat -N 5
%\end{alltt}

I now wish to launch longer runs on my cluster or powerful desktop. The syntax of the script depends on the cluster. In the simplest case it will only contain some general commands concerning the job name, wall time limit etc., and the command line above (I can use the one without input file, provided that I made already one short interactive run, and that the \verb?log.param? already exists; but I can now increase the number of steps, e.g. to 5000 or 10000). On some cluster, the chain file is created immediately in the output directory at start up. In this case, the automatic numbering of chains proposed by \MP~will be satisfactory. In other clusters, the chains are created on a temporary file, and then copied at the end to the output file. In this case, if I do nothing, there is a risk that chain names are identical and clash. I should then relate the chain name to the job number, with an additional command line \verb?-chain_number $JOBID?. Some clusters, \verb?$JOBID? is a string, but the job number can be extracted with a line like \verb?export JOBNUM="$(echo $PBS_JOBID|cut -d'.' -f1)"?, and passed to \MP~as  \verb?-chain_number $JOBNUM?.

If I use in a future run the WMAP likelihood, I should not forget to add in the script (before calling \MP) the line
\begin{alltt}
source /path/to/my/wrapper_wmap/bin/clik_profile.sh
\end{alltt}

I then launch a chain by submitting the script, with e.g. \verb?qsub scripts/lcdm.sh?. I can launch many chains in one command with
\begin{alltt}
\$ for i in \{1..10\}; do qsub scripts/lcdm.sh;done
\end{alltt} 
Advanced cluster users may even find ways to launch several serial runs appearing as a single job on the cluster with the \verb?mpirun? command.

When the runs have stopped, I can analyse them with
\begin{alltt}
\$ code/Montepython.py -info chains/planck/lcdm 
\end{alltt}
If I had been running without a covariance matrix, the results would probably
be bad, with a very low acceptance rate and few points. It would have however
created a covariance matrix \verb?chains/planck/lcdm/lcdm.covmat?. I can decide
to copy it in order to keep track of it even after analysing future runs, 
\begin{alltt}
cp chains/planck/lcdm/lcdm.covmat chains/planck/lcdm/lcdm_run1.covmat
\end{alltt}
I now add to my script, in the line starting with \verb?code/Montepyhton.py?, the option 
\begin{alltt}
-c chains/planck/lcdm/lcdm_run1.covmat
\end{alltt} and I launch a new run with several chains. If I launch this second
run on the same day as the previous one, it might be smart to change also a bit
the number of steps (e.g. from 5000 to 5001) in order to immediately identify
chains belonging to the same run.

When this second run is finished, I analyse it with e.g.
\begin{alltt}
code/Montepython.py -info chains/planck/lcdm/2012-10-27_5001*
\end{alltt}
If all R-1 numbers are small (typically $<0.05$) and plots look nice, I am
done. If not, there can be two reasons: the covariance matrix is still bad, or
I just did not get enough samples.

I can check the acceptance rate of this last run by looking at the
\verb?chains/planck/lcdm/lcdm.log? file. If I am in a case with nearly gaussian
posterior (i.e. nearly ellipsoidal contours), an acceptance rate $<0.2$ or
$>0.3$ can be considered as bad. In other cases, even 0.1 might be the best
that I can expect. If the acceptance rate is bad, I must re-run with an
improved covariance matrix in order to converge quicker. I copy the last
covariance matrix to \verb?lcdm_run2.covmat? and use this one for the next run.
If the acceptance rate is good but the chains are not well converged because
they are simply too short, then I should better rerun with the same covariance
matrix \verb?lcdm_run1.covmat?: in this way, I know that the proposal density
is frozen since the second run, and I can safely analyse the second and third
runs altogether.

If I do two or three runs in that way, I always loose running time, because
each new chain will have a new burn-in phase (i.e. a phase when the log
likelihood is very bad and slowly decreasing towards values close to the
minimum). If this is a concern, I can avoid it in three ways:
\begin{itemize}
\item before launching the new run, I set the input mean value of each
  parameter in the input file to the best-fit value found in the previous run.
  The runs will then start from the best-fit value plus or minus the size of
  the first jump drown from the covariance matrix, and avoid burn-in. Since I
  have changed the input file, I must rerun with a new output directory, e.g.
  \verb?chain/lcdm2?. This is a clean method.
\item I might prefer a less clean but slightly quicker variant: I modify the
  mean values, like in the previous item, but directly in the \verb?log.param?
  file, and I rerun in the same directory without an input file. This will
  work, but it is advisable not to edit the \verb?log.param? manually, since it
  is supposed to keep all the information from previous runs.
\item I may restart the new chains from previous chains using the \verb?-r?
  command line option. The name of previous chains can be written after
  \verb?-r? manually or through a script.
\end{itemize}

When I am pleased with the final plots and result, I can customize the plot
content and labels by writing a short file \verb?plot_files/lcdm.plot? passed
through the \verb?-extra? command line option, and paste the latex file
produced by \MP~in my paper.

\section{Existing and new likelihoods\label{likelihoods}}

\subsection{One likelihood = one directory, one .py and one .data file\label{ssec:lkl}}

We have seen already that cosmological parameters are passed directly from the
input file to \CLASS, and do not appear anywhere in the code itself, i.e. in
the files located in the \verb?code/? directory. The situation is the same for
likelihoods. You can write the name of a likelihood in the input file, and
\MP~will directly call one of the external likelihood codes implemented in the
\verb?likelihood/? directory. This means that when you add some new
likelihoods, you don't need to declare them in the code. You implement them in
the \verb?likelihood? directory, and they are ready to be used if mentioned in
the input file.

This can work because a precise syntax must be respected. Each likelihood is
associated to a name, e.g. \verb?hst?,  \verb?wmap?,  \verb?WiggleZ? (the name
is case-sensitive). This name is used:
\begin{itemize}
\item for calling the likelihood in the input file, e.g. \verb?data.experiments = ['hst', ...]?,
\item for naming the directory of the likelihood, e.g. \verb?likelihoods/hst/?,
\item for naming the input data file describing the characteristics of the experiment,  \verb?likelihoods/hst/hst.data? (this file can point to raw data files located in the \verb?data? directory)
\item for naming the python file containing the likelihood code,  \verb?likelihoods/hst/hst.py? 
\item for naming the class declared in \verb?likelihoods/hst/hst.py? and used also in \verb?likelihoods/hst/hst.data?
\end{itemize}
When  implementing new likelihoods, you will have to follow this rule. You
could already wish to have two Hubble priors/likelihoods in your folder. For
instance, the distributed version of \verb?hst? corresponds to a gaussian prior
with standard deviation $h=0.72\pm0.02$. If you want to change these numbers,
you can simply edit \verb?likelihoods/hst/hst.data? and change the numbers 0.72
and 0.02. But you could also keep \verb?hst? unchanged and create a new
likelihood called e.g. \verb?spitzer?. We will come back to the creation of
likelihoods later, but just to illustrate the structure of likelihoods, let us
see how to create such a prior/likelihood:
\begin{alltt}
\$ mkdir likelihoods/spitzer
\$ cp likelihoods/hst/hst.data likelihoods/spitzer/spitzer.data
\$ cp likelihoods/hst/hst.py likelihoods/spitzer/spitzer.py
\end{alltt}
Then edit \verb?likelihoods/spitzer/spitzer.py? and replace in the initial declaration the class name \verb?hst? by \verb?spitzer?:
\begin{alltt}
class spitzer(likelihood_prior):
\end{alltt}
Edit also \verb?likelihoods/spitzer/spitzer.data?, replace the class name \verb?hst? by \verb?spitzer?, and the numbers by your constraint:
\begin{alltt}
spitzer.h = 0.743
spitzer.sigma = 0.021
\end{alltt}
You are done. You can simply add \verb?data.experiments = [...,'spitzer', ...]? to the list of experiments in the input parameter file and the likelihood will be used.

\subsection{Existing likelihoods}

We release the first version of \MP~with the likelihoods:
\begin{itemize}
\item \verb?spt?, \verb?bicep?, \verb?cbi?, \verb?acbar?, \verb?bicep?, \verb?quad?, the latest public versions of CMB data from SPT, Bicep, CBI, ACBAR, BICEP and Quad; for the SPT likelihoods we include three nuisance parameters obeying to gaussian priors, like in the original SPT paper, and for ACBAR one nuisance parameter with top-hat prior. These experiments are described by the very same files as in a ComsoMC implementation. They are located in the \verb?data/? directory. For each experiment, there is a master file \verb?xxx.dataset? containing several variables and the names of other files with the raw data. In the files \verb?likelihoods/xxx/xxx.data?, we just give the name of the different \verb?xxx.dataset? files, that \MP~is able to read just like CosmoMC.
\item \verb?wmap?, original likelihood file accessed through the wmap wrapper. The file \verb?likelihoods/wmap/wmap.data? allows you to call this likelihood with a few different options (e.g. switching on/off Gibbs sampling, choosing the minimum and maximum multipoles to include, etc.) As usual, we implemented the nuisance parameter \verb?A_SZ? with a flat prior. In the input parameter file, you can decide to vary this parameter in the range 0-2, or to fix it to some value.
\item \verb?hst? is the HST Key Project gaussian prior on $h$,
\item \verb?sn? constains the luminosity distance-redhsift relation using the Union 2 data compilation,
\item \verb?WiggleZ? constraints the matter power spectrum $P(k)$ in four different redshift bins using recent WiggleZ data,
\end{itemize}
plus a few other likelihoods referring to future experiments, described in the next subsection. All these likelihoods are strictly equivalent to those in the CosmoMC patches released by the various experimental collaborations.

\subsection{Mock data likelihoods}

We also release simplified likelihoods \verb?fake_planck_bluebook?, \verb?euclid_lensing? and \verb?euclid_pk? for doing forecasts for Planck, Euclid (cosmic shear survey) and Euclid (redshift survey).

In the case of Planck, we use a simple gaussian likelihood for TT, TE, EE (like in \verb?astro-ph/0606227? with no lensing extraction) with sensitivity parameters matching the numbers published in the Planck bluebook. In the case of Euclid, our likelihoods and sensitivity parameters are specified in \verb?arXiv:1210.219?. The sensitivity parameters can always be modified by the user, by simply editing the \verb?.data? files.

These likelihoods compare theoretical spectra to a fiducial spectrum (and NOT to random data generated given the fiducial model: this approach is simpler and leads to the same forecast error bars, see \verb?astro-ph/0606227?).

Let us illustrate the way in which this works with \verb?fake_planck_bluebook?, although the two Euclid likelihoods obey exactly to the same logic.


When you download the code, the file \verb?likelihoods/fake_planck_bluebook/fake_planck_bluebook.data? has a field
\verb?fake_planck_bluebook.fiducial_file? pointing to the file\\
\verb?'fake_planck_bluebook_fiducial.dat'?. You downloaded this file together
with the code: it is located in \verb?data? and it contains the TT/TE/EE
spectrum of a particular fiducial model (with parameter values logged in the
first line of the file). If you launch a run with this likelihood, it will work
immediately and fit the various models to this fiducial spectrum.

But you probably wish to choose your own fiducial model. This is extremely
simple with \MP. You can delete the provided fiducial
file\\\verb?'fake_planck_bluebook_fiducial.dat'?, or alternatively,
you can change the name of the fiducial file in\\
\verb?likelihoods/fake_planck_bluebook/fake_planck_bluebook.data?. When you
start the next run, the code will notice that there is no input fiducial
spectrum. It will then generate one automatically, write it in the correct file
with the correct location, and stop after this single step. Then, you can
launch new chains, they will fit this fiducial spectrum. 

When you generate the fiducial model, you probably want to control exactly fiducial parameter values. If you start from an ordinary input file with no particular options, \MP~will perform one random jump and generate the fiducial model. Fiducial parameter values will be logged in the first line of the fiducial file. But you did not choose them yourself. However, when you call \MP~with the intention of generating a fiducial spectrum, you can pass the command line option \verb?-f 0?. This sets the variance of the proposal density to zero. Hence the fiducial model will have precisely the parameter values specified in the input parameter file. The fiducial file is even logged in the \verb?log.param? of all the runs that have been using it.

\subsection{Creating new likelihoods belonging to pre-defined category}

A likelihood is a class (let's call it generically \verb?xxx?), declared and
defined in \verb?likelihoods/xxx/xxx.py?, using input numbers and input files
names specified in \verb?likelihoods/xxx/xxx.data?. The actual data files
should usually be placed in the \verb?data/?folder (with the exception of WMAP
data). Such a class will always inherit from the properties of the most generic
class defined inside \verb?code/likelihoods_class.py?. But it may fall in the
category of some pre-defined likelihoods and inherit more properties. In this
case the coding will be extremely simple, you won't need to write a specific
likelihood code.

In the current version, pre-defined classes are:
\begin{itemize}
\item \verb?likelihood_newdat?, suited for all CMB experiments described by a file in the \verb?.newdat? format (same files as in CosmoMC).
\item \verb?likelihood_mock_cmb?, suited for all CMB experiments dexcribed with a simplified gaussian likelihood, like our  \verb?fake_planck_bluebook? likelihood.
\item \verb?likelihood_mpk?, suited for matter power spectrum data that would be described with a \verb?.dataset? file in CosmoMC. This generic likelihood contains a piece of code following closely the routine \verb?mpk? developped for CosmoMC. In the released version of \MP, this likelihood type is only used by each of the four redshift bins of the WiggleZ data, but it is almost ready for being used with other data set in this format.
\end{itemize}
Suppose, for instance, that a new CMB dataset \verb?nextcmb? is released in the \verb?.newdat? format. You will then copy the \verb?.newdat? file and other related files (with window functions, etc.) in the folder \verb?data/?. You will then create a new likelihood, starting from an existing one, e.g cbi:
\begin{alltt}
$ mkdir likelihoods/nextcmb
$ cp likelihoods/cbi/cbi.data likelihoods/nextcmb/nextcmb.data
$ cp likelihoods/cbi/cbi.py likelihoods/nextcmb/nextcmb.py
\end{alltt}
The python file should only be there to tell the code that nextcmb is in the \verb?.newdat? format. Hence it should only contain:
  \begin{alltt}
  from likelihood_class import likelihood_newdat 
  class nextcmb(likelihood_newdat):
    pass
  \end{alltt}
This is enough: the likelihood is fully defined. The data file should only contain the name of the \verb?.newdat? file:
  \begin{alltt}
nextcmb.data_directory  = data.path['data']
nextcmb.file            = 'next-cmb-file.newdat'
\end{alltt}
Once tou have edited these few  lines, you are done! No need to tell \MP~that there is a new likelihood! Just call it in your next run by adding \verb?data.experiments = [...,'nextcmb', ...]? to the list of experiments in the input parameter file, and the likelihood will be used.

You can also define nuisance parameters, contamination spectra and nuisance priors for this likelihood, as explained in the next section.

\subsection{Creating new likelihoods from scratch}

The likelihood \verb?sn? is an example of individual likelihood code: the actual code is explicitly written in \verb?sn.py?. To create your own likelihood files, the best to is look at such examples and follow them. We do not provide a full tutorial here, and encourage you to ask for help if needed. Here are however some general indications.

Your customised likelihood should inherit from generic likelihood properties through:
 \begin{alltt}
  from likelihood_class import likelihood
  class my-likelihood(likelihood):
  \end{alltt}

Implementing the likelihood amounts in developing in the python file \verb?my-likelihood.py? the properties of two essential functions, \verb?__init__? and \verb?loglkl?. But you don't need to code everything from scratch, because the generic likelihood already knows the most generic steps.

This means that you don't need to write from scratch the parser reading the \verb?.data? file: this will be done automatically at the beginning of the initialization of your likelihood. Consider that any field defined with a line in the \verb?.data? file, e.g. \verb?my-likelihood.variance = 5?, are known in the likelihood code: in this example you could write in the python code something like \verb?chi2+=result**2/self.variance?.

You don't need either to write from scratch an interface with \CLASS. You just need to write somewhere in the initialization function  some specific parameters that should be passed to \CLASS. For instance, if you need the matter power spectrum, write
\begin{alltt}
self.need_cosmo_arguments(data,{'output':'mPk'})
  \end{alltt}
  If this likelihood is used, the field \verb?mPk? will be appended to the list of output fields (e.g. \verb?output=tCl,pCl,mPk?), unless it was already there. If you write
\begin{alltt}
self.need_cosmo_arguments(data,{'l_max_scalars':3300})
   \end{alltt}
   the code will check if \verb?l_max_scalars? was already set at least to 3300, and if not, it will increase it to 3300. But if another likelihood needs more it will be more.
   
 You don't need to redefine functions like for instance those defining the role of nuisance parameters (especially for CMB experiments). 
 If you write in the \verb?.data? file
 \begin{alltt}
my-likelihood.use_nuisance           = ['N1','N2'] 
\end{alltt}
the code will know that this likelihood cannot work if these two nuisance parameters are not specified  in the parameter input file (they can be varying or fixed; fix them by writing a 0 in the sigma entry). If you try to run without them, the code will stop with an explicit error message.
If the parameter \verb?N1? has a top-hat prior, no need to write it: just specify prior edges in the input parameter file. If \verb?N2? has a gaussian prior, specify it in the \verb?.data? file, e.g.:
 \begin{alltt}
  my-likelihood.N2_prior_center  = 1
  my-likelihood.N2_prior_variance = 2  
\end{alltt}
Since these fields refer to pre-defined properties of the likelihood, you don't need to write explicitly in the code something like \verb?chi2 += (N2-center)**2/variance?, adding the prior is done automatically. Finally, if these nuisance parameters are associated to a CMB dataset, they may stand for a multiplicative factor in front of a contamination spectrum to be added to the theoretical $C_l$'s. This is the case for the nuisance parameters of the \verb?acbar?, \verb?spt? and \verb?wmap? likelihoods delivered with the code, so you can look there for concrete examples. To assign this role to these nuisance parameters, you just need to write
\begin{alltt}
  my-likelihood.N1_file = 'contamination_corresponding_to_N1.data'
\end{alltt}
and the code will understand what it should do with the parameter \verb?N1? and the file\\
\verb?data/contamination_associated_to_N1.data?. Optionally, the factor in front of the contamination spectrum can be rescaled by a constant number using the syntax:
\begin{alltt}
  my-likelihood.N1_scale = 0.5
\end{alltt}
To know exactly what is pre-defined in the generic likelihood class, open the file \verb?code/likelihood_class.py?. You will find the following set of functions:
  \begin{itemize}
    \item \verb?read_from_file?, to read your \verb?.data? file and extract
      properly the information
    \item \verb?get_cl?, to return the properly normalized $C_l$'s from \CLASS, in $\mu K^2$.
    \item \verb?need_cosmo_arguments?, to enforce the definition of a certain
      value of certain cosmo parameters (e.g. to enforce that the
      $C_l$'s get computed until a certain $l$),
    \item \verb?read_contamination_spectra?, 
    \item \verb?add_contamination_spectra?,
    \item \verb?add_nuisance_prior?.
  \end{itemize}

Creating new likelihoods requires a basic knowledge of python. If you are new in python, once you know the basics, you will realise how concise a code can be. You can compare the length of the likelihood codes that we provide with their equivalent in Fortran in the CosmoMC package.

\end{document}
