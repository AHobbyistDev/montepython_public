\documentclass[10pt]{article}
\usepackage{url}
\usepackage{alltt}
\usepackage{hyperref}
\usepackage{fullpage}
\newcommand{\CLASS}{\texttt{CLASS}}
\newcommand{\MP}{\texttt{Monte Python}}
\newcommand{\Wlik}{\texttt{Wlik}}
\newcommand{\Clik}{\texttt{Clik}}
\newcommand{\tild}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}


\title{ {\huge \bf Monte Python Documentation}\\Release v1.0}
\author{Benjamin Audren}

\begin{document}
\maketitle

\newpage

\tableofcontents

\newpage

\section{Installing prerequisites}

  \subsection{Python}

  First of all, you need a clean installation of {\bf python} (version $2.x$,
  $x\geq7$, but $<3.0$), with at least the {\bf numpy module} (version
  $\geq1.4.1$) and the {\bf cython module}. This last one is to convert the C
  code \CLASS~into a Python class.\par

  If you also want the output plot to have cubic interpolation for analyzing
  chains, you should have a full {\bf scipy module} (at least version $0.9.0$),
  with the method {\emph interpolate}. In case this one is badly installed, you
  will have an error message when running the analyze module of \MP,
  and obtain only linear interpolation. Though not fatal, it does produce
  uglier plots.\par

  Note that you can use the code with Python 2.6 also, even though you need to
  download two packages separately (ordereddict and argparse). For this, it is
  just a matter of downloading the two files (ordereddict.py and argparse.py),
  and placing them in your code directory.\par

  \subsection{\CLASS}

  Next in line, you must compile the python wrapper of \CLASS. Download the
  latest version at \url{http://class-code.net}, and follow the basic
  instruction. Instead of the \verb?make class?, do a \verb?make?. This will
  also create an archiv .ar of the code, useful in the next step. After this, do:

  \begin{alltt}
   class]\$ cd python/
   python]\$ python setup.py build
   python]\$ python setup.py install --user
  \end{alltt}

  If you have correctly installed cython, this should add Classy as a new python
  module. You can check the success of this operation by running the following
  command:

  \begin{alltt}
    \tild]\$ python
    >>> from classy import Class
  \end{alltt}

  If you get no error message from this line, you know everything is fine.

  \subsubsection*{Living with different versions of \CLASS}

  If at some point, you have several different coexisting versions of \CLASS~
  on the system, and you are worried that \MP~ is not using the good one, rest
  reassured. As long as you run \MP~ with the proper path to the proper \CLASS,
  then it will use this one (see section \ref{sec:installation}: Installation).

  \subsection{WMAP 7 likelihood}

  To use the likelihood of wmap, we propose a python wrapper, located in the
  \verb?wrapper_wmap? directory. As for the \CLASS wrapper, you will need to
  install it, although the procedure differs.

  Go to the wrapper directory, and enter:

  \begin{alltt}
    wrapper_wmap]\$ ./waf configure install_all_deps
  \end{alltt}

  This should read the configuration of you distribution, install the
  dependencies (cfitsio) automatically on your machine. For our purpose,
  though, we prefer using the intel mkl libraries, which are much faster. To
  tell the code of your local installation of mkl libraries, please use this
  syntax:

  \begin{alltt}
    wrapper_wmap]\$ ./waf configure --lapack_mkl=/path/to/intel/mkl/10.3.8 --lapack_mkl_version=10.3
  \end{alltt}

  Once the configuration is done properly, finalize the installation by typing:
  
  \begin{alltt}
    wrapper_wmap]\$ ./waf install
  \end{alltt}

  The code will generate a configuration file, that you will need to source
  before using it with \MP. The file is \verb?clik_profile.sh?, and is located
  in \verb?wrapper_wmap/bin/?. So before any call to \MP, or inside your
  scripts, if you want to use the likelihood \verb?`wmap'?, you should call

  \begin{alltt}
    \tild]\$ source /path/to/MontePython/wrapper_wmap/bin/clik_profile.sh
  \end{alltt}

\newpage
%%%%%%%% INSTALLATION %%%%%%%%
\section{Installation\label{sec:installation}}

  Move the latest release of \MP\, to your code/ folder, and untar its
  content:

  \begin{alltt} 
    code]\$ bunzip montepython-vx.y.z.tar.bz2 
    code]\$ tar -xvf montepython-vx.y.z.tar
    code]\$ cd montepython
  \end{alltt}

 You will have to edit two files (once for every new distribution of \MP, and
 the other one, once and for all). The first to edit is
 \emph{code/MontePython.py}. Its first line reads:

 \begin{alltt}
    #!/usr/bin/python
 \end{alltt}

 You have to change this line to wherever your python executable is installed.
 This modification is not crucial, since it will only affect the program if
 executed. If, instead, you run it through python (\emph{i.e.}: \verb?python?
 \verb?code/MontePython.py?), then this line will be disregarded.

 The second file to change, and this one is crucial, is {\bf default.conf}, in
 the root directory of the code. This file will tell \MP\, where your
 other programs are installed, and where you are storing the data for the
 likelihoods. It will be interpreted as a python file, so be careful to
 reproduce the syntax exactly.

 At minimum, {\bf default.conf} should contain one line, filled with the
 relevant information for your machine:
 \begin{alltt}
   path['cosmo']   = 'path/to/your/class/'
 \end{alltt}
 
 %For members of the Planck collaboration only: if you have installed Clik, then you should also add:
 %\begin{alltt}
 %  path['clik']    = 'path/to/your/clik/examples/'
 %\end{alltt}
 
 To run \MP, simply type \verb?python code/MontePython.py --help?. This will provide you with a short description of the available command line arguments. We will go below, in section blabla, over a typical run of the code.
 
\newpage
%%%%%%%% SUMMARIZED %%%%%%%%%%
\section{Summarized behaviour and philosophy of \MP}

  The principle is the following. You run \MP~ giving him, at least, a
  parameter file and an output folder. This will create a new folder, store the
  parameter file, the likelihood properties, the \CLASS~ version (in
  \emph{folder/log.param}), and start a Markov Chain there.  You can then
  launch new runs in this existing folder, as long as you are not using a different
  parameter file. Otherwise, the program will simply not launch. \\

  All the configuration files for all the likelihoods, cosmo parameters and
  mcmc parameters will be stored in this folder. This will allow you to make
  reference to this file, and rerun anything with the exact same settings as much
  as you want.\\

  After the analysis step described later, the folder will also contain more files:
  \begin{itemize}
    \item \emph{folder.v\_info}, \emph{folder.h\_info}:, \emph{folder.tex}:: with summarized convergence properties, mean and
      best-fit values, and 1-sigma deviations.
    \item \emph{folder.log}:  with the detailed properties of all files used to create
      folder.info
    \item \emph{folder.covmat}: contains the covariance matrix of the parameters
  \end{itemize}
  as well as a new subfolder: \emph{plots/}, with the triangle and 1d plot.



  \subsection{Parameter Files}
  This file contains the following lines:

  \begin{alltt}
    data.experiments = ['wmap','spt']
    
    data.parameters[cosmo_name]       = [mean,min,max,1-sigma prior,scale,'cosmo']
    ...

    data.parameters[nuisance_name]    = [mean,min,max,1-sigma prior,scale,'nuisance']
    ...

    data.parameters[cosmo_name]       = [mean,min,max,1-sigma prior,scale,'derived']
    ...

    data.cosmo_arguments[comso_name]           = value

    data.N = 10
    data.write_step = 5
  \end{alltt}

  The first command is rather explicit. You will list there all the experiments
  you want to take into account. The way they work will be explained in
  subsection \ref{ssec:lkl}\\

  In \verb?data.parameters?, you will list all the cosmo and nuisance parameter you
  want to vary (if they differ from the default values). You must give an array
  with six elements in it, in this order: 
  \begin{itemize}
    \item {\bf mean value}, 
    \item {\bf minimum value} (set to $-1$ if irrelevant or unimportant), 
    \item {\bf maximum value} (same), 
    \item {\bf 1-sigma prior} (only used when there are no covariance matrix as
      an input),
    \item {\bf scale} (most of the time, it will be $1$, but occasionnaly you can use a rescaling factor, for instance {\tt 1.e-9} if you are dealing with {\tt A\_s}) and finally 
    \item {\bf role} ({\tt 'cosmo'} for mcmc parameters used by the Boltzmann code, {\tt 'nuisance'} for MCMC parameters used only by the likelihoods, and {\tt 'related'} for parameters not varied by the MCMC algorithm, but to to be kept in the chains as derived parameters).
      \end{itemize}
  
  In  \verb?data.cosmo_arguments?, you can pass any parameter to the Boltzmann code that you want to fix to a non-default value (precision or cosmological parameters, instruction for using or not lensing, tensors, etc., input file needed by the Bolztmann code, etc.).
  What you
  pass here as argument is in fact the same value you would use in the
  \verb?explanatory.ini? of your \CLASS~ runs. So it is not an array, but either a
  number or a string.\\

  All elements you input with {\tt 'cosmo'} role will
  be interpreted by the cosmological code (only
  \CLASS~ so far). You may wish to use in the MCMC runs a new parameter that can be mapped to a \CLASS~parameter (e.g. $\log(10^{10})A_s$ instead of $A_s$). There is a function, located in \verb?code/data.py?, that you can edit to define such mappings. It is called  \verb?update_cosmo_arguments?. Before calling \CLASS, this function will simply replace in the list of arguments your customized parameters by true \CLASS~parameters. Several exemple of such mappings are
  alreay implemented, allowing you for instance to use $\Omega_{\Lambda}$ as an MCMC parameter.\\

  The last two elements are the number of steps you want your chain to contain
  (\verb?data.N?) and the number of accepted steps the system should wait
  before writing it down to a file (\verb?data.write_step?). Typically, you
  will need a rather low number here, if you are running on a cluster where
  your jobs might easily get killed, or when you are testing. If instead you
  are afraid of loosing time with writing on disk tasks, you can put it higher.
  Anyway, it is recommend not to go much higher than 5, for it really is not
  the most time consuming part of the code.\\

  {\bf Note}: you will want to overwrite the \verb?data.N? in the command line,
  with the option \verb?-N 10000?. The value by default in the parameter file
  is intentionnaly low (though you can change it of course). The reason is
  simply to prevent doing any mistake while testing the program on a cluster.

  \subsection{Folder = set of experiments and parameters}
  
  You are assumed to use the code in the following way: for every set of
  experiments and parameters you want to test, including different priors, some
  parameters fixed, etc\ldots you should use one folder. This way, the folder
  will keep track of the exact calling of the code, allowing you to reproduce
  the data at later times, or to complete the existing chains. All these
  important data are stored in your \verb?folder/log.param? file.\\

  Incidentaly, if you are starting the program in an existing folder, already
  containing a \verb?log.param? file, then you do not even have to specify a
  parameter file: the code will use it automatically. This will avoid mixing
  things up. If you are using one anyway, the code will first check whether the
  two parameter files are in agreement, and if not, will stop and say so.\\

  \subsection{Input covariance matrix}

  From any folder containing chains with more than 20 accepted points, you can
  launch the analyze part of the code by invoking a command like:
  \begin{alltt}
    python code/MontePython.py -info chains/wmap7_sn/ -bins 15
  \end{alltt}
  This will create, among other, a \verb?chains/wmap7_sn/wmap7_sn.covmat? file,
  that will contain on the first line all the names of the varying parameters
  of this run, and immediatly below the deduced covariance matrix.\\

  You can then feed this as an input for a new run, with the following command
  line argument :
  \begin{alltt}
    python code/MontePython.py -o new_folder -p new.param -c wmap7_sn/wmap7_sn.covmat
  \end{alltt}
  Here, \verb?new.param? could well have different parameters (for
  instance, using a varying $N_{eff}$ instead of a fixed one). The program
  will then deduce the correct starting covariance matrix of the already
  computed elements, only using the $1\sigma$ prior for new parameters. Do not
  bother to have the same ordering: the code is doing it all for you.

  \subsection{Likelihood = a .py and a .data file\label{ssec:lkl}}

  To finish this summarized presentation, let us look on the likelihood files.
  They are defined in a very specific way, that you will have to respect
  whenever you want to implement new ones.\\

  Every likelihood is defined in a
  \verb?path/to/MontePython/likelihoods/name_of_likelihood/?
  folder. This folder contains at least two files, \verb?name_of_likelihood.py?
  and \verb?name_of_likelihood.data?. It is crucial that the same name \verb?name_of_likelihood? is used for the folder,
  for the two files {\tt .data} and {\tt.py}, and for calling this likelihood in the Monte Python input file.\\

  The \verb?.py? file defines the class (in python sense) of your likelihood.
  Fortunately for you, parent classes have already been defined in the
  \verb?code/likelihood_class.py?. For instance, for all the newdat type of
  likelihoods (acbar, boomerang, bicep, etc.), their \verb?.py? file will be
  painfully simple:

  \begin{alltt}
  from likelihood_class import likelihood_newdat

  class acbar(likelihood_newdat):
    pass
  \end{alltt}

  Yes, that is all. This created a new class \verb?acbar? inheriting properties from the \verb?likelihood_newdat? class. Again it is crucial that the name of the class is the same as the name of the folder and of the two files {\tt .data} and {\tt.py}.
    Now you have to fill in the \verb?.data? file. Continuing
  on the example with acbar, which requires a nuisance parameter (either fixed
  or varying), let us take a look at the entirity of the file
  \verb?acbar.data?:
  
  \begin{alltt}
    acbar.data_directory = data.path['data']
    acbar.file           = 'acbar2007_v3_corr.newdat'

    acbar.use_nuisance   = ['A_SZ']
    acbar.A_SZ_file      = 'WMAP_SZ_VBand.dat'
    acbar.A_SZ_scale     = 0.28
  \end{alltt}

  The only particular thing is there: the \verb?.use_nuisance? option. You have
  to pass here the entire list of the nuisance parameters the likelihood is
  expecting before running. After, for each nuisance parameter, you need to
  specify a few quantities that we will detail later.  Here, you could also have a line that
  force Class to have certain parameters (a certain $l_{max}$ for instance). To
  enforce this behaviour, just add:
  \begin{alltt}
    acbar.need_cosmo_arguments(data,\{'l_max_scalars':3300\})
  \end{alltt}

  Now in your parameter file, simply input the line \verb?data.exp=['acbar']?,
  and the code will compute the new likelihood. You have a new experiment,
  already in the newdat format ? Create the folder, the two files, put your
  data in \verb?data.path['data']? and the name in the list of experiments and
  here you go. It really is this simple.
  
  If you are adding an experiment which does not match with a pre-defined category like \verb?likelihood_newdat? or \verb?likelihood_mpk?, its class should at lease inherit from basic likelihood properties:
    \begin{alltt}
  from likelihood_class import likelihood

  class my_favorite_experiment(likelihood):
    
  \end{alltt}

  and below you should write explicitly the likelihood code in python, with an initialisation step and a likelihood computation step: you can start from one of the several examples we distribute with the code.
  

  \subsection{General guidelines to keep in mind}

  Here follows some rules about being sure everything work fine with \MP.
\begin{itemize}
  \item Be sure to launch only one experiment by folder. If, after some month,
    you want to relaunch a run with the same liklihood, simply call the program without any
    parameter file, but just the existing output folder. The likelihood characteristics were stored for you in the \verb?log.param?.
%  \item When launching a new run by extending a previous one (adding a free
 %   $N_{\textrm{eff}}$ to $\Lambda CDM$ for instance), it is often better not to
  %  take any starting covariance matrix than one that might not be adapted.
  \item when launching a new run from uncertain priors or from a bad covariance matrix,
    use the \verb?-j sequential? flag. It creates much more correlated chains,
    but this helps get started, produce a meaningful covariance matrix, and
    start again from a better point.
\end{itemize}
\newpage
\section{Creating (completely new) likelihoods}

  We already saw that adding likelihoods that already come into a certain format
  (from Clik, or with the newdat format) is a matter of three minutes. However,
  if one ones to code a new likelihood, one has to understand a bit the way
  classes work in this code.\\

  The file \verb?code/likelihood_class.py? contains all the basic definitions
  of likelihoos already implemented. The basic one, \verb?likelihood? will be
  useful to you if you want to create a new one from scratch. Just let your new
  class inherit from likelihood, and build over that. The following functions are already coded for you:\\

  \begin{itemize}
    \item \verb?read_from_file?, it will only read your .data file and extract
      properly the information
    \item \verb?get_cl?, returns the properly normalized $C_l$ from Class, in $\mu K^2$.
    \item \verb?need_cosmo_arguments?, enforce the definition at a certain
      value of certain cosmo parameters. Typically, this will enforce that the
      $C_l$ gets computed until a certain $l$.
    \item \verb?read_contamination_spectra?, TODO julien ?
    \item \verb?add_contamination_spectra?,
    \item \verb?add_nuisance_prior?,
  \end{itemize}


  \newpage
  \section{Example of a complete work session with \MP}
  Let us get through a typical work session with this code. We start from the
  example.param file. We will test here a $\Lambda$CDM model on the likelihoods
  of the Hubble Space Telescope (HST) and the South Pole Telescope (SPT)
  experiments.\\

  To this end, our example.param file will contain as first lines the following:
  
  \begin{alltt}
    data.experiments = ['hst','spt']
  \end{alltt}

  Moreover, by looking at the file \verb?likelihoods/spt/spt.data?, one can see
  that this particular likelihood requires three nuisance parameters, be them
  fixed or varying: this file contains the following line:
  \verb?spt.use_nuisance	   = ['SPT_SZ','SPT_PS','SPT_CL']?
\end{document}
