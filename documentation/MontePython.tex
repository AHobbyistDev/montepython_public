\documentclass[10pt]{article}
\usepackage{url}
\usepackage{alltt}
\usepackage{hyperref}
\usepackage{fullpage}
\newcommand{\CLASS}{\texttt{CLASS}}
\newcommand{\MP}{\texttt{Monte Python}}
\newcommand{\Wlik}{\texttt{Wlik}}
\newcommand{\Clik}{\texttt{Clik}}
\newcommand{\tild}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}


\title{ {\huge \bf Monte Python Documentation}\\Release v1.0}
\author{Benjamin Audren \& Julien Lesgourgues}

\begin{document}
\maketitle

\newpage

\tableofcontents

\newpage

\section{Prerequisites}

  \subsection{Python}

  First of all, you need a clean installation of {\bf python} (version $2.x$,
  $x\geq7$, but $<3.0$), with at least the {\bf numpy module} (version
  $\geq1.4.1$) and the {\bf cython module}. This last one is to convert the C
  code \CLASS~into a Python class.\par

  If you also want the output plot to have cubic interpolation for analyzing
  chains, you should also have the {\bf scipy module} (at least version $0.9.0$).
  %with the method \emph{interpolate}. 
  In case this one is badly installed, you
  will have an error message when running the analyze module of \MP,
  and obtain only linear interpolation. Though not fatal, this problem produces
  ugly plots.\par

To test for the presence of the modules {\bf numpy},  {\bf scipy},  {\bf cython} on your machine, you can type
  \begin{alltt}
   \$ python
   \$ >>> import numpy
   \$ >>> import scipy
   \$ >>> import cython
   \$ >>> exit()
  \end{alltt}
 If one of these steps fails, go to the corresponding websites, and follow the instructions (if you have the honour to have the root password on your machine, an \verb?apt-get install python-numpy?, \verb?python-scipy? and \verb?cython? will do the trick.).

  Note that you can use the code with Python 2.6 also, even though you need to
  download two packages separately ({\bf ordereddict} and {\bf argparse}). For this, it is
  just a matter of downloading the two files (\verb?ordereddict.py? and \verb?argparse.py?),
  and placing them in your code directory without installation steps.\par

  \subsection{\CLASS}

  Next in line, you must compile the python wrapper of \CLASS. Download the
  latest version ($\geq 1.5.0$) at \url{http://class-code.net}, and follow the basic
  instruction. Instead of  \verb?make class?, type \verb?make?. This will
  also create an archiv .ar of the code, useful in the next step. After this, do:

  \begin{alltt}
   class]\$ cd python/
   python]\$ python setup.py build
   python]\$ python setup.py install --user
  \end{alltt}

  If you have correctly installed cython, this should add Classy as a new python
  module. You can check the success of this operation by running the following
  command:

  \begin{alltt}
    \tild]\$ python
    >>> from classy import Class
  \end{alltt}

If the installation was successfull, this should work within any directory. If you get no error message from this line, you know everything is fine.

  \subsubsection*{Living with different versions of \CLASS}

  If at some point you have several different coexisting versions of \CLASS~on the system, and you are worried that \MP~is not using the good one, rest
  reassured. As long as you run \MP~with the proper path to the proper \CLASS~in your configuration file (see section \ref{sec:installation}: Installation),
  then it will use this one.

  \subsection{WMAP 7 likelihood}

  To use the likelihood of WMAP, we propose a python wrapper, located in the
  \verb?wrapper_wmap? directory. Just like with the \CLASS~wrapper, you need to
  install it, although the procedure differs. Go to the wrapper directory, and enter:

  \begin{alltt}
    wrapper_wmap]\$ ./waf configure install_all_deps
  \end{alltt}

  This should read the configuration of your distribution, and install the WMAP likelihood code and its
  dependencies (cfitsio) automatically on your machine. For our purpose,
  though, we prefer using the intel mkl libraries, which are much faster. To
  tell the code about your local installation of mkl libraries, please add to the line above some options:
  \begin{alltt}
     --lapack_mkl=/path/to/intel/mkl/10.3.8 --lapack_mkl_version=10.3
  \end{alltt}

  Once the configuration is done properly, finalize the installation by typing:
  
  \begin{alltt}
    wrapper_wmap]\$ ./waf install
  \end{alltt}

  The code will generate a configuration file, that you will need to source
  before using the WMAP likelihood with \MP. The file is \verb?clik_profile.sh?, and is located
  in \verb?wrapper_wmap/bin/?. So if you want to use the likelihood \verb?`wmap'?, before any call to \MP~(or inside your
  scripts), you should execute

  \begin{alltt}
    \tild]\$ source /path/to/MontePython/wrapper_wmap/bin/clik_profile.sh
  \end{alltt}

\newpage
%%%%%%%% INSTALLATION %%%%%%%%
\section{Installation\label{sec:installation}}

  Move the latest release of \MP~to one of your folders, called e.g. \verb?code/? (for instance, this could be the folder containing also \verb?class/?), and untar its
  content:

  \begin{alltt} 
    code]\$ bunzip montepython-v1.0.0.tar.bz2 
    code]\$ tar -xvf montepython-v1.0.0.tar
    code]\$ cd montepython
  \end{alltt}

 You will have to edit two files (the first, once for every new distribution of \MP, and
 the second, once and for all). The first to edit is
 \verb?code/MontePython.py?. Its first line reads:

 \begin{alltt}
    #!/usr/bin/python
 \end{alltt}

 You should eventually replace this path with the one of your python 2.7 executable, if different.
 This modification is not crucial, it simply allows to run the code by simply typing \verb?code/Montepython.py?.
 If, instead, you run it through python (\emph{i.e.}: \verb?python?
 \verb?code/MontePython.py?), then this line will be disregarded.

 The second file to change, and this one is crucial, is \verb?default.conf?, in
 the root directory of the code. This file will tell \MP\, where your
 other programs (in particular \CLASS) are installed, and where you are storing the data for the
 likelihoods. It will be interpreted as a python file, so be careful to
 reproduce the syntax exactly. At minimum, {\bf default.conf} should contain one line, filled with the path of your \verb?class/? directory:
 \begin{alltt}
   path['cosmo']   = 'path/to/your/class/'
 \end{alltt}
 
 %For members of the Planck collaboration only: if you have installed Clik, then you should also add:
 %\begin{alltt}
 %  path['clik']    = 'path/to/your/clik/examples/'
 %\end{alltt}
 
 To check that \MP~is ready to wrok, simply type \verb?python code/MontePython.py --help? (or just \verb?code/MontePython.py --help?). This will provide you with a short description of the available command line arguments, explained in section \ref{commands}. 

\section{Getting started}

\subsection{Input parameter file}

An example of input parameter file is provided with the download package, under the name \verb?example.param?. Input files are organised as follows:

  \begin{alltt}
    data.experiments = ['experiment1', 'experiment2', ...]
    
    data.parameters['cosmo_name']       = [mean, min, max, sigma, scale, 'cosmo']
    ...

    data.parameters['nuisance_name']    = [mean, min, max, sigma, scale, 'nuisance']
    ...

    data.parameters['cosmo_name']       = [mean, min, max, sigma, scale, 'derived']
    ...

    data.cosmo_arguments['cosmo_name']           = value

    data.N = 10
    data.write_step = 5
  \end{alltt}

  The first command is rather explicit. You will list there all the experiments
  you want to take into account. Their name should coincide with the name of one of the several sub-directories in the \verb?likelihood/? directory. Likelihoods will be explained in section \ref{likelihoods}\\

  In \verb?data.parameters?, you can list all the cosmo and nuisance parameter that you
  want to vary in the Markov chains. For each of them you must give an array
  with six elements, in this order: 
  \begin{itemize}
    \item {\bf mean value} (your guess for the best fitting value, from which the first jump will start)
    \item {\bf minimum value} (set to $-1$ for unbounded prior edge), 
    \item {\bf maximum value} (set to $-1$ for unbounded prior edge), 
    \item {\bf sigma} (your guess for the standard deviation of the posterior of this parameter, its square will be used as the variance of the proposal density when there is no covariance matrix including this parameter passed as an input),
    \item {\bf scale} (most of the time, it will be $1$, but occasionnaly you can use a rescaling factor for convenience, for instance {\tt 1.e-9} if you are dealing with \verb?A_s? or \verb?0.01? if you are dealing with \verb?omega_b?) 
    \item {\bf role} ({\tt 'cosmo'} for MCMC parameters used by the Boltzmann code, {\tt 'nuisance'} for MCMC parameters used only by the likelihoods, and {\tt 'derived'} for parameters not directly varied by the MCMC algorithm, but to be kept in the chains for memory).
      \end{itemize}
  
  In  \verb?data.cosmo_arguments?, you can pass to the Boltzmann code any parameter that you want to fix to a non-default value (cosmological parameter, precision parameter, flag, name of input file needed by the Bolztmann code, etc.). The names and values should be the same as in a \CLASS~input file, so the values can be numbers or a strings, e.g:
  \begin{alltt}
    data.cosmo_arguments['Y_He']           = 0.25
    \end{alltt}  
    or
  \begin{alltt}
    data.cosmo_arguments['Y_He']           = 'BBN'    
    data.cosmo_arguments['sBBN file'] = data.path['cosmo']+'/bbn/sBBN.dat'
    \end{alltt}
  
  All elements you input with a \verb?cosmo?, \verb?derived? or \verb?cosmo_arguments? role will
  be interpreted by the cosmological code (only \CLASS~ so far). They are not coded anywhere inside \MP. \MP~takes parameter names, assigns values, and passes all of these to \CLASS~as if they were written in a \CLASS~input file. The advantages of this scheme are obvious. If you need to fix or vary whatever parameter known by \CLASS, you don't need to edit \MP, you only need to write these parameters in the input parameter file. Also, \CLASS~is able to interpret input parameters from a \CLASS~input file with a layer of simple logic, allowing to specify different parameter combinations. Parameters passed from the parameter file of \MP~go through the same layer of logic. 
  
  If a \verb?cosmo?, \verb?derived? or \verb?cosmo_arguments? parameter is not understood by the Boktzmann code, \MP~will stop and return an explicit error message. A similar error will occur if one of the liklihoods requires a \verb?nuisance? parameter that is not passed in the list.

  You may wish occasionally to use in the MCMC runs a new parameter that is not a \CLASS~ parameter, but can be mapped to one or several \CLASS~parameters (e.g. you may wish to use in your chains $\log(10^{10}A_s)$ instead of $A_s$). There is a function, located in \verb?code/data.py?, that you can edit to define such mappings. It is called  \verb?update_cosmo_arguments?. Before calling \CLASS, this function will simply substitute in the list of arguments your customized parameters by some \CLASS~parameters. Several exemple of such mappings are already implemented, allowing you for instance to use 
\verb?'Omega_Lambda'?, \verb?'ln10^{10}A_s'? or \verb?'exp_m_2_tau_As'? in your chains. Looking at these examples, the user can easily write new ones even without knowing python.
  
  The last two lines of the input parameter file are the number of steps you want your chain to contain
  (\verb?data.N?) and the number of accepted steps the system should wait
  before writing it down to a file (\verb?data.write_step?). Typically, you
  will need a rather low number here, e.g. \verb?data.write_step = 5? or \verb?10?. The reason for not setting this parameter to one is just to save a bit of time in writing on the disk.
 
  In general, you will want to specify the number of steps in the command line,
  with the option \verb?-N? (see section~\ref{commands}). This will overwrite the value passed in the input parameter file. The value by default in the parameter file, \verb?data.N = 10?,
  is intentionnaly low, 
  simply to prevent doing any mistake while testing the program on a cluster.


\subsection{Output directory}

You are assumed to use the code in the following way: for every set of
  experiments and parameters you want to test, including different priors, some
  parameters fixed, etc\ldots you should use one output folder. This way, the folder
  will keep track of the exact calling of the code, allowing you to reproduce
  the data at later times, or to complete the existing chains. All 
  important data are stored in your \verb?folder/log.param? file.\\

  Incidentaly, if you are starting the program in an existing folder, already
  containing a \verb?log.param? file, then you do not even have to specify a
  parameter file: the code will use it automatically. This will avoid mixing
  things up. If you are using one anyway, the code will first check whether the
  two parameter files are in agreement, and if not, will stop and say so. In that way, you are sure not to put together some chains obtained under different physical assumptions.\\
  
  In the folder \verb?montepyhton?, you can create a folder \verb?chains? where you will organize your runs e.g. in the following way:
    \begin{alltt}
    montepython/chains/set_of_experiments1/model1
    montepython/chains/set_of_experiments1/model2
    ...
        montepython/chains/set_of_experiments2/model1
            montepython/chains/set_of_experiments2/model2    
  ...
  \end{alltt}
  
  The minimum amount of command lines for running \MP~is an input file, an output directory and a configuration file: if you have already edited \verb?defaut.conf? or copied it to your own \verb?my-machine.conf?, you may already try a mini-run with the command 
    \begin{alltt}
    montepython]$ code/MontePython.py -conf my-machine.conf -p example.param -o test
    \end{alltt}

\subsection{Analyzing chains and plotting}

Once you have accumulated a few chains, you can analyse the run to get convergence estimates, best-fit values, minimum credible intervals, a covariance matrix  and some plots of the marginalised posterior probability. You can run again \MP~with the \verb?-info? prefix followed by the name of a directory or of several chains, e.g.
\verb?-info chains/myrun/? or \verb?-info chains/myrun/2012-10-26* chains/myrun/2012-10-27*?.
There is no need to pass an input file with parameter names since they have all been stores in the \verb?log.param?.

Information on the acceptance rate and minimum $-\log{\cal L}=\chi^2_{\rm eff}/2$ is written in \verb?chains/myrun/myrun.log?. Information on the convergence (Raferty \& Lewis test for each chain paramater), on the best fit, mean and minimum credible interval for each parameter at the 68.26\%, 95.4\%, 99.7\% level are written in horizontal presentation in \verb?chains/myrun/myrun.h_info?, and in vertical presentation in \verb?chains/myrun/myrun.v_info? (without 99.7\% in the vertical one). A latex file to produce a table with parameter names, means and 68\% errors in written in \verb?chains/myrun/myrun.tex?.

The covariance matrix of the run is written in \verb?chains/myrun/myrun.covmat?. It can be used as an input for the proposal density in a future run. The first line, containing the parameter name, will be read when the covariance matrix will be passed in input. This means that 
the list of parameters in the input covariance matrix and in the run don't need to coincide: the code will automatically eliminate, add and reorder parameters. Note that the rescaling factors passed in the input file are used internally during the run and also in the presentation of results in the \verb?.h_info?, \verb?.v_info?, \verb?.tex? files, but not in the covariance matrix file, which refers to the true parameters.

The 1D posteriors and 2D posterior contours are plotted in \verb?chains/myrun/plots/myrun_1D.pdf? and \verb?chains/myrun/plots/myrun_triangle.pdf?. You will find in section~\ref{commands} a list of commands to customize the plots. Besides, you may wish to change font sizes: for the moment this has to be done by editing the very last lines of \verb?code/analyse.py? and writing your own values for the \verb?fontsize? and \verb?ticksize? variables.

When the chains are not very converged and the posterior probability has local maxima, the code will fail to compute minimum credible intervals and say it in a warning. The two solutions are either to re-run and increase the number of samples, or maybe just to decrease the number of bins with the \verb?-bins? option.

\subsection{Global running strategy}
     
In the current version of \MP, we deliberately  choose not to use MPI communication between instances of the code. Indeed the use of MPI usually makes the installation step more complicated, and the gain is, in our opinion, not worth it. Several chains are launched as individual serial runs (if each instance of \MP~is launched on several cores, \CLASS~and the WMAP likelihood will parallelize since they use OpenMP). They can be run with the same command since chain names  are created automatically with different numbers for each chain: the chain names are in  the form
\verb?yyyy-mm-dd_N__i.txt? where {\tt yyy} is the year, {\tt mm} the month, {\tt dd} the day, {\tt N} the requested number of steps and {\tt i} the smallest available integer at the time of starting a new run.

However the absence of communication between chains implies that the proposal density cannot be updated automatically during the initial stage of a run. Hence the usual strategy consists in launching a first run with a poor (or no) covariance matrix, and a low acceptance rate; then to analyze this run and produce a better covariance matrix; and then to launch a new run with high acceptance rate, leading to nice plots. Remember that in order to respect strictly markovianity and the Metropolis Hastings algorithm, one should not mix up chains produced with different covariance matrices: this is easy if one takes advantage of the \verb?-info? syntax, for example \verb?-info chains/myrun/2012-10-26_10000*?. However mixing runs that started from very similar covariance matrices is harmless.

It is also possible to run on several desktops instead of a single cluster. Each desktop should have a copy of the output folder and with the same \verb?log.param? file, and after running the chains can be grouped on a single machine and analyse. In this case, take care of avoiding that chains are produced with the same name (easy to ensure with either the \verb?-N? or \verb?-chain_number? options). This is a good occasion to keep the desktops of your department finally busy.
     
\section{Command line arguments\label{commands}}

The command \verb?code/MontePython.py --help? lists all command line arguments understood by \MP. They fall in two categories:
\begin{itemize}
\item arguments  useful for running chains:\\
\begin{tabular}{ll}
\verb?-N steps? & number of steps in the chain (when running on a cluster,\\& your run might be stopped before reaching this number)\\
\verb?-o output_folder? &for example \verb?-o chains/myexperiments/mymodel?\\
\verb?-p input_param_file?&for example \verb?-p input/exoticmodel.param?\\
\verb?-c input_cov_matrix?& name of a covariance matrix (created when analyzing a previous run,\\&and used in input to initialise the proposal density). \\&List of parameters in the input covariance matrix \\&and in the run don't need to coincide.\\
\verb?-j jumping_method? & can take two values, \verb?global? (default) and \verb?sequential?.\\& With the global method the code generates a new random direction \\& at each step, with the sequential one it cycles over the eigenvectors of \\&the proposal density (= input covariance matrix). With the global method\\& the acceptance rate is usually lower but the points in the chains are less\\& correlated.  We recommend using the sequential method to get started in\\& difficult cases, when the proposal density is very bad, in order to \\& accumulate points and generate a covariance matrix to be used later \\&with the default jumping method.\\
\verb?-f jumping_factor? & the proposal density is given by the input covariance matrix \\&(or a diagonal matrix with elements given by the square of the input \\&sigma's) multiplied by the square of this factor. In other words, a \\& typical jump will have an amplitude given by sigma times this factor. \\&The default is the famous factor 2.4, found by Dunkely et al. to be\\& an optimal trade-off between high acceptance rate and high correlation\\& of chain elements, at least for multivariate gaussian posterior probabilities.\\& It can be a good idea to reduce this factor for very non-gaussian posteriors. \\&Using \verb?-f 0 -N 1? is a convenient way to get the likelihood exactly at the \\& starting point passed in input. \\
\verb?-conf configuration_file? & as explained before, this command is crucial to tell \MP\\&about the location of \CLASS.\\
\verb?-chain_number chain_number? & chains are named automatically \verb?yyyy-mm-dd_N__i.txt? where {\tt yyy}\\& is the year, {\tt mm} the month, {\tt dd} the day, {\tt N} the requested number of steps\\& and {\tt i} the smallest available integer at the time of starting a new run:\\& so running \MP~several times with exactly the same command\\& will automatically lead to different chain names. This option is a way to \\&enforce a particular number {\tt i}. This can be useful when running on a\\& cluster: for instance you may ask your script to use the job number as {\tt i}. \\
\verb?-r chain_name? & restart from the last point of a previous chain, to avoid a new\\& burn-in stage. At the beginning of the run, the previous chain\\& will be deleted, and its content transfered to the beginning of\\& the new chain.
\end{tabular}
\item arguments  useful for analyzing chains and plotting:\\
\begin{tabular}{ll}
\verb?-info [file [file ...]]?& chains to analyze; give the folder name to analyze all chains in this folder,\\& or individual chains, for example \verb?-info chains/myrun/?\\& or \verb?-info chains/myrun/2012-10-26* chains/myrun/2012-10-27*?\\
\verb?-bins number_of_bins? &  number of bins in the histograms used to derive posterior probabilities\\& and credible intervals, default is 20; reduce this number for smoother\\& plots at the expense of masking details\\
\verb?-no_mean? &by default, when plotting marginalised 1D posteriors, the code also shows the mean likelihood per bin with dashed lines; this option switches off the dashed lines\\
\verb?-comp comparison_folder? & pass the name of another folder (or another set of chains, same syntax as \verb?-info?) if you want to compare 1D posteriors on the same plot. The lists of parameters in the two folders to compare do not need to coincide. Limited so far to two folders to compare in total.\\
\verb?-extra plot_file? & name of an optional file containing a few lines for customizing the plots,\\& \verb?info.to_change={'oldname1':'newname1','oldname2':'newname2',...}?\\&
\verb?info.to_plot=['name1','name2','newname3',...]?\\& 
\verb?infot.new_scales={'name1':number1,'name2':number2,...}?\\
\verb?-noplot? & analyze chains without drawing plots \\
\verb?-all? & by default, produces only full 1D plot and triangle plots, with this option\\& all individual 1D plot and 2D contour plot are stored 
\end{tabular}
\end{itemize}

\section{Example of a complete work session}

I just downloaded and installed \MP, read the previous pages, and I wish to launch and analyse my first run.

I can first create a few folders in order to keep my \verb?montepython? directory tidy in the future. I do a \\
\begin{tabular}{ll}
\verb?$ mkdir chains? &for storing all my chains\\
\verb?$ mkdir chains/planck?& if the first run I want to launch is based on the fake planck likelihood proposed\\& in the \verb?example.param? file\\
\verb?$ mkdir input?& for storing all my input files\\
\verb?$ mkdir scripts?& for storing all my scripts for running the code in batch mode\\
\verb?$ mkdir covmat?&to copy later some reference covariance matrices\\
\verb?$ mkdir plot_files?&to store files used to customize plots
\end{tabular}\\

I then copy \verb?example.param? in my input folder, with a name of my choice, e.g. \verb?lcdm.param?, and edit it if needed:
\begin{alltt}
$ cp example.param input/lcdm.param
\end{alltt}
I then launch a short chain with
 \begin{alltt}
$ code/Moncode/Montepython.py -conf my-conf.conf -p input/lcdm.param\\ \mbox{    }-o chains/planck/lcdm -N 5
\end{alltt}
I can see on the screen the evolution of the initialization of the code. At the end I check that I have a 
chain and a \verb?log.param? written in my \verb?chains/planck/lcdm/log.param? directory. I can immediately repeat the experience with the same command. The second chain is automatically created with number 2 instead of 1. I can also run again without the input file:
\begin{alltt}
$ code/Moncode/Montepython.py -conf my-conf.conf  -o chains/planck/lcdm -N 5
\end{alltt}
This works equally well because all information is taken from the \verb?log.param? file.
I don't have yet a covariance matrix to pass in input\footnote{If I am also a CosmoMC user, I might have an adequate covmat to start with, before using the covmat that \MP~will produce. Fot this I just need to edit the first line, add comas between paramater names, and for parameter that are identical to those in my run, replace CosmoMC parameter names with equivalent \CLASS~parameter names.}, otherwise I would have run with 
\begin{alltt}
$ code/Moncode/Montepython.py -conf my-conf.conf -p input/lcdm.param\\ \mbox{    } -o chains/planck/lcdm -c mycovmat.covmat -N 5
\end{alltt}

I now wish to launch longer runs on my cluster or powerful desktop. The syntax of the script depends on the cluster. In the simplest case it will only contain some general commands concerning the job name, wall time limit etc., and the command line above (I can use the one without input file, provided that I made already one short interactive run, and that the \verb?log.param? already exists; but I can now increase the number of steps, e.g. to 5000 or 10000). On some cluster, the chain file is created immediately in the output directory at start up. In this case, the automatic numbering of chains proposed by \MP~will be satisfactory. In other clusters, the chains are created on a temporary file, and then copied at the end to the output file. In this case, if I do nothing, there is a risk that chain names are identical and clash. I should then relate the chain name to the job number, with an additional command line \verb?-chain_number $JOBID?. Some clusters, \verb?$JOBID? is a string, but the job number can be extracted with a line like \verb?export JOBNUM="$(echo $PBS_JOBID|cut -d'.' -f1)"?, and passed to \MP~as  \verb?-chain_number $JOBNUM?.

If I use in a future run the WMAP likelihood, I should not forget to add in the script (before calling \MP) the line
\begin{alltt}
source /path/to/my/wrapper_wmap/bin/clik_profile.sh
\end{alltt}

I then launch a chain by submitting the script, with e.g. \verb?qsub scripts/lcdm.sh?. I can launch many chains in one command with
\begin{alltt}
$ for i in {1..10}; do qsub scripts/lcdm.sh;done
\end{alltt} 
Advanced cluster users may even find ways to launch several serial runs appearing as a single job on the cluster with the \verb?mpirun? command.

When the runs have stopped, I can analyse them with
\begin{alltt}
$ code/Montepython.py -info chains/planck/lcdm 
\end{alltt}
I have been running  without a covariance matrix, so the results are probably bad with a very low acceptance rate and few points. However I now have a covariance matrix \verb?chains/planck/lcdm/lcdm.covmat?. I can decide to copy it in order to keep track of it even after analysing future runs, 
\begin{alltt}
cp chains/planck/lcdm/lcdm.covmat chains/planck/lcdm/lcdm_run1.covmat
\end{alltt}
I now add to my script, in the line starting with \verb?code/Montepyhton.py?, the option 
\begin{alltt}
-c chains/planck/lcdm/lcdm_run1.covmat
\end{alltt} and I launch a new run with several chains. If I launch this second run on the same day as the previous one, it might be smart to change also a bit the number of steps (e.g. from 5000 to 5001) in order to immediately identify chains belonging to the same run.

When this second run is finished, I analyse it with e.g.
\begin{alltt}
code/Moncode/Montepython.py -info chains/planck/lcdm/2012-10-27_5001*txt 
\end{alltt}
If all R-1 numbers are small (typically $<0.05$) and plots look nice, I am done. If not, there can be two reasons: the covariance matrix is still bad, or I just did not get enough samples.

I can check the acceptance rate of this last run by looking at the \verb?chains/planck/lcdm/lcdm.log? file. If I am in a case with nearly gaussian posterior (i.e. nearly ellipsoidal contours), an acceptance rate $<2$ or $>3$ can be considered as bad. In other cases, even 0.1 might be the best that I can expect. If the acceptance rate is bad, I must re-run with an improved covariance matrix in order to converge quicker. I copy the last covariance matrix to \verb?lcdm_run2.covmat? and use this one for the next run. If the acceptance rate is good but the chains are not well converged because they are simply too short, then I should better rerun with the same covariance matrix \verb?lcdm_run1.covmat?: in this way, I know that the proposal density is frozen since the second run, and I can safely analyse the second and third runs altogether.

If I do two or three runs in that way, I always loose running time, because each new chain will have a new burn-in phase (i.e. a phase when the log likelihood is very bad and slowly decreasing towards values close to the minimum). If this is a concern, I can avoid it in three ways:
\begin{itemize}
\item before launching the new run, I set the input mean value of each parameter in the input file to the best-fit value found in the previous run. The runs will then start from the best-fit value plus or minus the size of the first jump drown from the covariance matrix, and avoid burn-in. Since I have changed the input file, I must rerun with a new output directory, e.g. \verb?chain/lcdm2?. This is a clean method.
\item I might prefer a less clean but slightly quicker variant: I modify the mean values, like in the previous item, but directly in the \verb?log.param? file, and I rerun in the same directory without an input file. This will work, but it is advisable not to edit the \verb?log.param? manually, since it is supposed to keep all the information from previous runs.
\item I may restart the new chains from previous chains using the \verb?-r? command line option. The name of previous chains can be written after \verb?-r? manually or through a script.
\end{itemize}

When I am pleased with the final plots and result, I can customize the plot content and labels by writing a short file \verb?plot_files/lcdm.plot? passed through the \verb?-extra? command line option, and paste the latex file produced by \MP~in my paper.

\section{Existing and new likelihoods\label{likelihoods}}


  \subsection{Likelihood = a .py and a .data file\label{ssec:lkl}}

  To finish this summarized presentation, let us look on the likelihood files.
  They are defined in a very specific way, that you will have to respect
  whenever you want to implement new ones.\\

  Every likelihood is defined in a
  \verb?path/to/MontePython/likelihoods/name_of_likelihood/?
  folder. This folder contains at least two files, \verb?name_of_likelihood.py?
  and \verb?name_of_likelihood.data?. It is crucial that the same name \verb?name_of_likelihood? is used for the folder,
  for the two files {\tt .data} and {\tt.py}, and for calling this likelihood in the Monte Python input file.\\

  The \verb?.py? file defines the class (in python sense) of your likelihood.
  Fortunately for you, parent classes have already been defined in the
  \verb?code/likelihood_class.py?. For instance, for all the newdat type of
  likelihoods (acbar, boomerang, bicep, etc.), their \verb?.py? file will be
  painfully simple:

  \begin{alltt}
  from likelihood_class import likelihood_newdat

  class acbar(likelihood_newdat):
    pass
  \end{alltt}

  Yes, that is all. This created a new class \verb?acbar? inheriting properties from the \verb?likelihood_newdat? class. Again it is crucial that the name of the class is the same as the name of the folder and of the two files {\tt .data} and {\tt.py}.
    Now you have to fill in the \verb?.data? file. Continuing
  on the example with acbar, which requires a nuisance parameter (either fixed
  or varying), let us take a look at the entirity of the file
  \verb?acbar.data?:
  
  \begin{alltt}
    acbar.data_directory = data.path['data']
    acbar.file           = 'acbar2007_v3_corr.newdat'

    acbar.use_nuisance   = ['A_SZ']
    acbar.A_SZ_file      = 'WMAP_SZ_VBand.dat'
    acbar.A_SZ_scale     = 0.28
  \end{alltt}

  The only particular thing is there: the \verb?.use_nuisance? option. You have
  to pass here the entire list of the nuisance parameters the likelihood is
  expecting before running. After, for each nuisance parameter, you need to
  specify a few quantities that we will detail later.  Here, you could also have a line that
  force Class to have certain parameters (a certain $l_{max}$ for instance). To
  enforce this behaviour, just add:
  \begin{alltt}
    acbar.need_cosmo_arguments(data,\{'l_max_scalars':3300\})
  \end{alltt}

  Now in your parameter file, simply input the line \verb?data.exp=['acbar']?,
  and the code will compute the new likelihood. You have a new experiment,
  already in the newdat format ? Create the folder, the two files, put your
  data in \verb?data.path['data']? and the name in the list of experiments and
  here you go. It really is this simple.
  
  If you are adding an experiment which does not match with a pre-defined category like \verb?likelihood_newdat? or \verb?likelihood_mpk?, its class should at lease inherit from basic likelihood properties:
    \begin{alltt}
  from likelihood_class import likelihood

  class my_favorite_experiment(likelihood):
    
  \end{alltt}

  and below you should write explicitly the likelihood code in python, with an initialisation step and a likelihood computation step: you can start from one of the several examples we distribute with the code.
  

  \subsection{General guidelines to keep in mind}
 Be sure to launch only one experiment by folder. If, after some month,
    you want to relaunch a run with the same liklihood, simply call the program without any
    parameter file, but just the existing output folder. The likelihood characteristics were stored for you in the \verb?log.param?.

\subsection{Creating (completely new) likelihoods}

  We already saw that adding likelihoods that already come into a certain format
  (from Clik, or with the newdat format) is a matter of three minutes. However,
  if one ones to code a new likelihood, one has to understand a bit the way
  classes work in this code.\\

  The file \verb?code/likelihood_class.py? contains all the basic definitions
  of likelihoos already implemented. The basic one, \verb?likelihood? will be
  useful to you if you want to create a new one from scratch. Just let your new
  class inherit from likelihood, and build over that. The following functions are already coded for you:\\

  \begin{itemize}
    \item \verb?read_from_file?, it will only read your .data file and extract
      properly the information
    \item \verb?get_cl?, returns the properly normalized $C_l$ from Class, in $\mu K^2$.
    \item \verb?need_cosmo_arguments?, enforce the definition at a certain
      value of certain cosmo parameters. Typically, this will enforce that the
      $C_l$ gets computed until a certain $l$.
    \item \verb?read_contamination_spectra?, TODO julien ?
    \item \verb?add_contamination_spectra?,
    \item \verb?add_nuisance_prior?,
  \end{itemize}


\end{document}
